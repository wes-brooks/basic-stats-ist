---
title: Introduction to basic statistics with R
author: [Dr. Wesley Brooks]
output:
   rmdformats::readthedown
date: "2021-02-11"
url: "https://ucdavisdatalab.github.io/adventures_in_data_science/"
github-repo: ucdavisdatalab/workshop_introduction_to_the_command_line
site: bookdown::bookdown_site
knit: "bookdown::render_book"
---

# Overview

Placeholder


## `mice_pot` dataset
## `barnacles` dataset
## Sample and population

<!--chapter:end:index.Rmd-->


# Probability

Placeholder


## Random variables and calculations:
### Mean and median
### Expectation
### Variance
### Some identities
## Some distributions
### Normal distribution
#### Distribution function
#### Density function
#### Quantile function
### Identities
### Law of large numbers
### Central limit theorem
#### But isn't $\sigma$ unknown, too?
## Revisiting the standard normal distribution:
## Looking at real data
### Normal data: using the t-distribution
#### Find the 80% confidence interval for the population mean
### Non-normal data: resampling
#### Bootstrap-t distribution:
## Links
## Background

<!--chapter:end:01_probability.rmd-->

# Mathematical statistics

## Identities
For random variables X and Y that are independent,
 - $$E(aX) = aE(X)$$
 - $$E(X + Y) = E(X) + E(Y)$$
 - $$var(aX) = a^2 var(X)$$
 - $$var(X + Y) = var(X) + var(Y)$$

```{r}
X = function(n=10, mean=-3, sd=3) { rnorm(n, mean=mean, sd=sd) }
Y = function(n=10, mean=10, sd=2) { rnorm(n, mean=mean, sd=sd) }

mean( X() )
mean( Y() )

var( X() )
var( Y() )

var( 2*X() )
var( X() + 0.5*Y() )
```


## Law of large numbers
The law of large numbers says that if the individual measurements are independent, then the mean of a sample tends toward the mean of the population as the sample size gets larger.

```{r}
nn = c(1, 2, 4, 8, 12, 20, 33, 45, 66, 100)
means = sapply( nn, function(n) mean( rnorm(n) ) )
plot(nn, means, bty='n', ylab = "sample mean")
abline(h=0, lty=2)
```


## Central limit theorem
The most important mathematical result in statistics, the Central Limit Theorem says that if you take (almost) any sample of random numbers and calculate its mean, the distribution of the mean tends toward a normal distribution. We illustrate the "tending toward" with an arrow and it indicates that the distribution of a sample mean is only *approximately* Normal. But if the samples were from a Normal distribution then the sample mean has an *exactly* Normal distribution.

$$ \bar{X} \rightarrow N(\mu, \frac{\sigma^2}{n}) $$
And because of the identities we learned before, you can write this as 
$$\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \rightarrow N(0, 1) $$

In fact, the This is significant because we can use the standard normal functions on the right, and the data on the left, to start answering questions like, "what is the 95% confidence interval for the population mean?" 

### But isn't $\sigma$ unknown, too?
I've presented calculations that use the parameter $\sigma$. But in reality, $\sigma$ is just as unknown as $\mu$. Is it safe to replace $\sigma$ in the calculations by the standard deviation that we calculate from the sample using the `sd()` function? Not quite. If the $S$, the sample standard deviation (this is what you get from R's `sd()` function) is used for $\sigma$ in the CLT, then the distribution will be slightly less clustered at the central point. This distribution with slightly heavier tails gets called the "t-distribution", and its discovery was a triumph of early modern statistics. 

$$\frac{\bar{X} - \mu}{S/\sqrt{n}} \rightarrow t_{n-1} $$

```{r}
# plot the densities of a standard normal distribution, and of a t-distribution with five degrees of freedom:

plot( x=t, y=dnorm(t), ylim=c(0, 0.5), bty='n', lty=2, type='l', ylab="density" )
par( new=TRUE )
plot( x=t, y=dt(t, df=5), ylim=c(0, 0.5), xaxt='n', yaxt='n', xlab='', ylab='', bty='n', col='red', type='l' )
```

Once again, if the original samples were from a Normal distribution, then the distribution of the mean is exactly a $t$ distribution, but otherwie the $t$ distribution is an approximation that gets better as the sample size increases.

What's more important than memorizing the precise formula is to remember that for (almost) any data where the samples are independent, the mean comes from some distribution that is more and more like the Normal distribution as the sample size increases. Let's take a look at an example using the uniform distribution.

```{r}
# generate 20 samples from a uniform distribution and plot their histogram
N = 20
u = runif( N )
hist( u )

# generate 100 repeated samples of the same size, calculate the mean of each one, and plot the histogram of the means.
B = 100
uu = numeric( B )
for ( i in 1:B ) {
  uu[[i]] = mean( runif(N) )
}

hist(uu)

# what happens as B and N get larger and smaller? Do they play different roles?
```

<!--chapter:end:02_mathematical-statistics.rmd-->


# Inference

Placeholder


## Revisiting the standard normal distribution:
## Looking at real data
### Normal data: using the t-distribution
#### Find the 80% confidence interval for the population mean
### Non-normal data: resampling
#### Bootstrap-t distribution:

<!--chapter:end:03_inference.rmd-->


# Introduction

Placeholder


## Packages
## Random variables and calculations:
### Mean and median
### Expectation
### Variance
### Some identities
## Some distributions
### Normal distribution
#### Distribution function
#### Density function
#### Quantile function
### Identities
### Law of large numbers
### Central limit theorem
#### But isn't $\sigma$ unknown, too?
## Revisiting the standard normal distribution:
## Looking at real data
### Normal data: using the t-distribution
#### Find the 80% confidence interval for the population mean
### Non-normal data: resampling
#### Bootstrap-t distribution:
## Links
## Background

<!--chapter:end:README.rmd-->

